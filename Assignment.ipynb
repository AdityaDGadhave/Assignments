{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b3f8b-bdd0-4a52-8fb2-279c732efa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q1 Write an efficient algorithm that searches for a value target in an m x n integer matrix. This matrix has the following properties:\n",
    "Integers in each row are sorted from right to left.\n",
    "The first integer of each row is greater than the last integer of the previous row.\n",
    "                              Example-: \n",
    "                                        Input: matrix = [[1,3,5,7],[10,11,16,20],[23,30,34,60]], target = 3\n",
    "                                         Output: True'''\n",
    "\n",
    "\n",
    "\n",
    "def searchMatrix(matrix, target):\n",
    "    if not matrix or not matrix[0]:\n",
    "        return False\n",
    "\n",
    "    rows, cols = len(matrix), len(matrix[0])\n",
    "    low, high = 0, rows * cols - 1\n",
    "\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        mid_value = matrix[mid // cols][mid % cols]\n",
    "\n",
    "        if mid_value == target:\n",
    "            return True\n",
    "        elif mid_value < target:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "\n",
    "    return False\n",
    "\n",
    "# Example usage:\n",
    "matrix = [[1, 3, 5, 7], [10, 11, 16, 20], [23, 30, 34, 60]]\n",
    "target = 3\n",
    "result = searchMatrix(matrix, target)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52e8c0-30f8-4c6d-b68d-c5ce2868e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q2 2.  Write a program that takes a string as input, and counts the frequency of each word in the string, there might  be repeated \n",
    "characters in the string. Your task is to\n",
    " find the highest frequency and returns the length of the  highest-frequency word. \n",
    "\n",
    "Note - You have to write at least 2 additional test cases in which your program will run successfully and provide  an explanation for the same.  \n",
    "\n",
    "Example input - string = “write write write all the number from from from 1 to 100” \n",
    "\n",
    "Example output - 5 \n",
    "\n",
    "Explanation - From the given string we can note that the most frequent words are “write” and “from” and  the maximum value of both the values \n",
    "is “write” and its corresponding length is 5 '''\n",
    "\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def highest_frequency_word_length(s):\n",
    "    words = s.split()\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    if not word_counts:\n",
    "        return 0\n",
    "\n",
    "    max_frequency = max(word_counts.values())\n",
    "    most_frequent_words = [word for word, count in word_counts.items() if count == max_frequency]\n",
    "\n",
    "    max_length = max(len(word) for word in most_frequent_words)\n",
    "    return max_length\n",
    "\n",
    "\n",
    "input_string = \"write write write all the number from from from 1 to 100\"\n",
    "result = highest_frequency_word_length(input_string)\n",
    "print(result)\n",
    "\n",
    "input_string_1 = \"apple orange banana apple orange apple banana banana\"\n",
    "\n",
    "result_1 = highest_frequency_word_length(input_string_1)\n",
    "print(result_1)\n",
    "\n",
    "\n",
    "input_string_2 = \"programming python coding programming coding coding coding\"\n",
    "\n",
    "result_2 = highest_frequency_word_length(input_string_2)\n",
    "print(result_2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d601b83-b9c3-4885-9e97-b922aac86328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning\n",
    "\n",
    "\n",
    "'''Q1 1. Imagine you have a dataset where you have different Instagram features like u sername , Caption , Hashtag , Followers , \n",
    "Time_Since_posted , and likes , now your task is to predict the number of likes and Time Since posted and the rest of the features are your\n",
    " input features. Now you have to build a model which can predict the number of likes and Time Since posted. \n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"instagram_reach.csv\")\n",
    "\n",
    "# Select relevant features and target variables\n",
    "features = data[['Username', 'Caption', 'Hashtag', 'Followers', 'Time_Since_Posted']]\n",
    "target_likes = data['Likes']\n",
    "target_time_since_posted = data['Time_Since_Posted']\n",
    "\n",
    "# Convert categorical features like 'Username', 'Caption', and 'Hashtag' to numerical using techniques like one-hot encoding or label encoding\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "features_train, features_test, target_likes_train, target_likes_test, target_time_train, target_time_test = train_test_split(\n",
    "    features, target_likes, target_time_since_posted, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "features_train_scaled = scaler.fit_transform(features_train)\n",
    "features_test_scaled = scaler.transform(features_test)\n",
    "\n",
    "# Build and train the model for predicting 'Likes'\n",
    "model_likes = XGBRegressor()\n",
    "model_likes.fit(features_train_scaled, target_likes_train)\n",
    "\n",
    "# Make predictions on the test set for 'Likes'\n",
    "predictions_likes = model_likes.predict(features_test_scaled)\n",
    "\n",
    "# Evaluate the model for 'Likes'\n",
    "mse_likes = mean_squared_error(target_likes_test, predictions_likes)\n",
    "r2_likes = r2_score(target_likes_test, predictions_likes)\n",
    "\n",
    "print(f'Mean Squared Error (Likes): {mse_likes}')\n",
    "print(f'R-squared (Likes): {r2_likes}')\n",
    "\n",
    "# Build and train the model for predicting 'Time_Since_Posted'\n",
    "model_time_since_posted = XGBRegressor()\n",
    "model_time_since_posted.fit(features_train_scaled, target_time_train)\n",
    "\n",
    "# Make predictions on the test set for 'Time_Since_Posted'\n",
    "predictions_time_since_posted = model_time_since_posted.predict(features_test_scaled)\n",
    "\n",
    "# Evaluate the model for 'Time_Since_Posted'\n",
    "mse_time_since_posted = mean_squared_error(target_time_test, predictions_time_since_posted)\n",
    "r2_time_since_posted = r2_score(target_time_test, predictions_time_since_posted)\n",
    "\n",
    "print(f'Mean Squared Error (Time Since Posted): {mse_time_since_posted}')\n",
    "print(f'R-squared (Time Since Posted): {r2_time_since_posted}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d965f994-68ee-42e0-8844-8d7f15ceafba",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q2 2. \n",
    "Explain how you can implement ML in a real world application.\n",
    "\n",
    "Train an SVM regressor on : Bengaluru housing dataset\n",
    "\n",
    "                  Must include in details:\n",
    "\n",
    "                           - EDA\n",
    "\n",
    "                            - Feature engineering'''\n",
    "\n",
    "#EDA\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Bengaluru_House_Data.csv\")\n",
    "\n",
    "# Explore basic information about the dataset\n",
    "print(data.info())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Explore the target variable distribution (Price)\n",
    "sns.histplot(data['price'], kde=True)\n",
    "plt.title('Distribution of House Prices')\n",
    "plt.show()\n",
    "\n",
    "# Explore relationships between features and the target variable\n",
    "sns.scatterplot(x='total_sqft', y='price', data=data)\n",
    "plt.title('Relationship between Total Square Feet and Price')\n",
    "plt.show()\n",
    "\n",
    "# Explore categorical features like 'location'\n",
    "sns.countplot(x='location', data=data)\n",
    "plt.title('Distribution of Houses by Location')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Featuare Engineering\n",
    "\n",
    "# Handle missing values\n",
    "data['size'].fillna('2 BHK', inplace=True)\n",
    "data['bath'].fillna(data['bath'].median(), inplace=True)\n",
    "data['balcony'].fillna(data['balcony'].median(), inplace=True)\n",
    "\n",
    "# Extract the number of bedrooms from the 'size' column\n",
    "data['num_bedrooms'] = data['size'].apply(lambda x: int(x.split(' ')[0]))\n",
    "\n",
    "# Convert categorical variables like 'location' into numerical using one-hot encoding or label encoding\n",
    "data = pd.get_dummies(data, columns=['location'], drop_first=True)\n",
    "\n",
    "# Drop irrelevant or redundant columns\n",
    "data.drop(['size', 'society', 'availability', 'location', 'area_type'], axis=1, inplace=True)\n",
    "\n",
    "# Check the updated dataset\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# Train SVM regressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data.drop('price', axis=1)\n",
    "y = data['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train an SVM Regressor\n",
    "svm_regressor = SVR(kernel='linear')\n",
    "svm_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = svm_regressor.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218840eb-ed87-446b-a3db-2c63a4c51563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning\n",
    "\n",
    "'''Q1 Train a Pure CNN with less than 10000 trainable parameters using the MNIST Dataset having minimum validation accuracy of 99.40% \n",
    "Note - Code comments should be given for proper code understanding.'''\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# Expand dimensions to add a channel dimension (for CNN input)\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build a simple CNN model with less than 10,000 trainable parameters\n",
    "model = models.Sequential()\n",
    "\n",
    "# Convolutional layers\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten layer to transition from convolutional to dense layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Dense layers\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))  # Dropout layer for regularization\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test Accuracy: {test_acc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713dc32-15a2-42e9-8d99-f8c5a9b12f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2 Explain how you can implement DL in a real-world application.\n",
    "\n",
    "''' Problem Definition:\n",
    "\n",
    "Clearly articulate the problem you want to solve using DL.\n",
    "Define specific tasks like image classification, object detection, natural language processing, or anomaly detection.\n",
    "2. Data Collection and Preparation:\n",
    "\n",
    "Gather a large, high-quality dataset relevant to the problem.\n",
    "Clean and preprocess data to handle missing values, inconsistencies, and noise.\n",
    "Split data into training, validation, and testing sets.\n",
    "3. Model Selection:\n",
    "\n",
    "Choose a suitable DL architecture based on the problem type and data characteristics:\n",
    "Convolutional neural networks (CNNs) for image and spatial data.\n",
    "Recurrent neural networks (RNNs) for sequential data like text or time series.\n",
    "Transformers for advanced natural language processing tasks.\n",
    "Autoencoders for anomaly detection or dimensionality reduction.\n",
    "Generative adversarial networks (GANs) for image generation or data augmentation.\n",
    "4. Model Training:\n",
    "\n",
    "Feed the training data into the chosen model.\n",
    "Train the model using an optimization algorithm (e.g., gradient descent) to adjust its parameters.\n",
    "Monitor training progress and adjust hyperparameters as needed.\n",
    "5. Evaluation:\n",
    "\n",
    "Assess the model's performance on the validation set using appropriate metrics (e.g., accuracy, precision, recall, F1-score).\n",
    "Identify potential issues like overfitting or underfitting.\n",
    "6. Deployment:\n",
    "\n",
    "Integrate the trained model into a real-world application or system.\n",
    "Consider model size, inference speed, hardware compatibility, and ethical implications.\n",
    "7. Monitoring and Maintenance:\n",
    "\n",
    "Continuously monitor the model's performance in production.\n",
    "Retrain or adapt the model as needed to address changes in data or requirements.\n",
    "Key Considerations for Real-World DL:\n",
    "\n",
    "Data quality and quantity: DL models often require extensive datasets for effective training.\n",
    "Computational resources: Training DL models can be computationally intensive, requiring powerful hardware (GPUs or TPUs).\n",
    "Interpretability and explainability: Understanding how DL models make decisions can be challenging, raising concerns about bias and fairness.\n",
    "Ethical implications: Be mindful of potential biases in data and algorithms, and ensure responsible use of DL to avoid discrimination or harm.'''\n",
    "\n",
    "# code\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc5e582-197f-4e86-a2d9-68f873c9809d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
